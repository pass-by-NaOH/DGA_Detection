{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据加载和数据处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['corebot', 'alexa', 'ranbyus', 'symmi', 'emotet', 'dircrypt',\n       'matsnu', 'simda', 'fobber', 'pushdo', 'qadars', 'kraken',\n       'ramnit', 'nymaim', 'pykspa', 'tinba', 'murofet', 'cryptolocker',\n       'ramdo', 'vawtrak', 'conficker', 'padcrypt', 'rovnix', 'suppobox',\n       'necurs', 'gozi'], dtype=object)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "filename = 'dataset/dga_domains_sample.csv'\n",
    "# 数据导入\n",
    "df = pd.read_csv(filename, encoding=\"UTF-8\")\n",
    "# 设置标签\n",
    "df.columns = ['Label', 'Source', 'Domain']  # 为每一行命名\n",
    "# 来源的分类\n",
    "df['Source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 0,  1,  1, ...,  8,  1, 16], dtype=int64),\n Index(['corebot', 'alexa', 'ranbyus', 'symmi', 'emotet', 'dircrypt', 'matsnu',\n        'simda', 'fobber', 'pushdo', 'qadars', 'kraken', 'ramnit', 'nymaim',\n        'pykspa', 'tinba', 'murofet', 'cryptolocker', 'ramdo', 'vawtrak',\n        'conficker', 'padcrypt', 'rovnix', 'suppobox', 'necurs', 'gozi'],\n       dtype='object'))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替换为数字\n",
    "source_dict = pd.factorize(df['Source']) # source_dict[0]表示分类后的类别，source_dict[1]表示类别名称"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   Label  Source                   Domain\n0      1       0  cvyh1po636avyrsxebwbkn7\n1      0       1              plasticbags\n2      0       1                 mzltrack\n3      0       1                miss-slim\n4      1       2           txumyqrubwutbb",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>Source</th>\n      <th>Domain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>cvyh1po636avyrsxebwbkn7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>plasticbags</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>mzltrack</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>miss-slim</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>txumyqrubwutbb</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Source'] = source_dict[0]\n",
    "# 将合法域名标签改为1，dga的改为0\n",
    "df['Label'] = df['Label'].replace({'legit': 0, 'dga': 1})\n",
    "# 对于域名也进行处理，留下中间的可识别部分作为n-gram的输入\n",
    "domain = np.array(df['Domain'])\n",
    "D = []\n",
    "for url in domain:\n",
    "    # 删除.后的所有数据\n",
    "    url = url.split(\".\", 1)[0]\n",
    "    D.append(url)\n",
    "col_2 = ['Domain']\n",
    "df['Domain'] = pd.DataFrame(D, columns=col_2)\n",
    "# 其中Label中，1代表DGA域名，Source中1代表alexa域名\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据转化为N-gram向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        , ...,  3.87298335,\n         4.54223904,  0.68018308],\n       [ 0.        ,  0.        ,  0.        , ..., 12.12864022,\n        10.22623406,  5.67044117],\n       [ 0.        ,  0.        ,  0.        , ..., 18.54091216,\n        14.94764474,  9.37127608],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  3.65578756,\n         8.89735028,  4.04385943],\n       [ 0.        ,  0.        ,  0.        , ..., 13.34905642,\n        13.51432493, 17.43338993],\n       [ 0.        ,  0.        ,  0.        , ...,  4.04385943,\n         8.31641171, 13.44365341]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras_preprocessing import sequence\n",
    "import nltk\n",
    "\n",
    "# 形成列表\n",
    "list_Domain = df['Domain'].tolist()\n",
    "list_s = df['Source'].tolist()\n",
    "\n",
    "# 生成Bigram\n",
    "Bigram_Data = []\n",
    "\n",
    "for i in range(len(list_Domain)):\n",
    "    var = []\n",
    "    for j in range(len(list_Domain[i]) - 1):\n",
    "        var.append(list_Domain[i][j] + list_Domain[i][j + 1])\n",
    "    Bigram_Data.append(var)\n",
    "\n",
    "Bigram_D = []\n",
    "\n",
    "for i in range(len(list_Domain)):\n",
    "    for j in range(len(list_Domain[i]) - 1):\n",
    "        var = list_Domain[i][j] + list_Domain[i][j + 1]\n",
    "        Bigram_D.append(var)\n",
    "\n",
    "# 使用nltk统计bigram出现的次数\n",
    "\n",
    "freq_dist_Data = nltk.FreqDist(Bigram_D)\n",
    "\n",
    "\n",
    "\n",
    "# 用bigram出现的频率替换为数字\n",
    "def data_to_vector(freq_dist, Bigram_list, list_Label):\n",
    "    # 使用构造好的字典对域名进行处理\n",
    "    list_data = [[freq_dist[y] for y in x] for x in Bigram_list]\n",
    "    max_data_len = len(max(Bigram_list, key=len, default=''))\n",
    "    # 以里面域名最大长度构造特征，小于最大长度的用一个非常小的值填充\n",
    "    PAD_VALUE = 1e-10\n",
    "    data_vector = sequence.pad_sequences(list_data, maxlen=max_data_len, dtype=np.float, value=PAD_VALUE)\n",
    "    # 开平方根处理，使域名向量不会过大而导致无法输入神经网络，加60能够保留出现次数低的特征\n",
    "    data_vector = np.sqrt(data_vector + 60) - np.sqrt(PAD_VALUE + 60)\n",
    "    # 将标签列表转化为ndarray\n",
    "    list_label = np.array(list_Label)\n",
    "    return data_vector, list_label,max_data_len\n",
    "\n",
    "data_vector, list_label, max_data_len = data_to_vector(freq_dist_Data, Bigram_Data, list_Label)\n",
    "data_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络的设计"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow_core.python.keras.layers import Embedding, LSTM, Dropout, Flatten, Dense\n",
    "from tensorflow_core.python.keras import Input, Model\n",
    "\n",
    "\n",
    "# 按照参考论文设计\n",
    "def example_LSTM(max_features, max_data_len, class_num):\n",
    "    # 输入层\n",
    "    input_layer = Input(shape=(max_data_len,), dtype='int32')\n",
    "    # 词嵌入\n",
    "    embed_layer = Embedding(input_dim=max_features, output_dim=128, input_length=max_data_len)(input_layer)\n",
    "    # LSTM层\n",
    "    lstm = LSTM(128)(embed_layer)\n",
    "    # 丢弃50%\n",
    "    dropout = Dropout(0.5)(lstm)\n",
    "    # 全连接层\n",
    "    flat = Flatten()(dropout)\n",
    "    # 使用softmax做多分类问题\n",
    "    out = Dense(class_num, activation='softmax')(flat)\n",
    "    # 整合神经网络\n",
    "    model = Model(input_layer, out)\n",
    "\n",
    "    # 评价函数后得到所有的参数\n",
    "    METRICS = [\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    "\n",
    "    # 模型使用多元交叉熵损失函数（二分类问题），优化器使用Adam优化器，评价函数参照以上\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
